{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/venv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import fairseq\n",
    "\n",
    "import soundfile as sf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch_optimizer import AdaBound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "  \"model\": \"wav2vec2+AASIST\",\n",
    "  \"batch_size\": 8,\n",
    "  \"wandb_project\": \"project_name\",\n",
    "  \"d_args\": {\n",
    "      \"nb_samp\": 64600,\n",
    "      \"first_conv\": 128,\n",
    "      \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
    "  },\n",
    "  \"device\": \"cuda:0\",\n",
    "  \"num_class\": 2,\n",
    "  \"gpu_id\": 0,\n",
    "  \"num_capsules\": 30,\n",
    "  \"epoches\": 40,\n",
    "  \"opt\": \"AdaBound\",\n",
    "  \"lr\": 0.0001,\n",
    "  \"weight_decay\": 0.00001,\n",
    "  \"random\": True,\n",
    "  \"dropout\": 0.05,\n",
    "  \"random_size\": 0.01,\n",
    "  \"num_iterations\": 2,\n",
    "  \"gamma\": 0.5,\n",
    "  \"step_size\": 10,\n",
    "  \"produced_file\": \"ssl_preds.txt\",\n",
    "  \"asv_score_filename\": \"/asvspoof/LA/ASVspoof2019_LA_asv_scores/ASVspoof2019.LA.asv.dev.gi.trl.scores.txt\",\n",
    "  \"dev_label_path\": \"/asvspoof/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.dev.trl.txt\",\n",
    "  \"dev_path_flac\": \"/asvspoof/LA/ASVspoof2019_LA_dev\",\n",
    "  \"train_label_path\": \"/asvspoof/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt\",\n",
    "  \"train_path_flac\":\"/asvspoof/LA/ASVspoof2019_LA_train\",\n",
    "  \"eval_label_path\": \"/asvspoof/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt\",\n",
    "  \"eval_path_flac\": \"/asvspoof/LA/ASVspoof2019_LA_eval\",\n",
    "  \"checkpoint\": \"/app/SafeSpeak-2024/weights/LA_model.pth\",\n",
    "  \"num_workers\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "___author__ = \"Hemlata Tak\"\n",
    "__email__ = \"tak@eurecom.fr\"\n",
    "\n",
    "############################\n",
    "## FOR fine-tuned SSL MODEL\n",
    "############################\n",
    "\n",
    "\n",
    "class SSLModel(nn.Module):\n",
    "    def __init__(self,device):\n",
    "        super(SSLModel, self).__init__()\n",
    "        \n",
    "        cp_path = '/app/SafeSpeak-2024/weights/xlsr2_300m.pt'   # Change the pre-trained XLSR model path. \n",
    "        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([cp_path])\n",
    "        self.model = model[0]\n",
    "        self.device=device\n",
    "        self.out_dim = 1024\n",
    "        return\n",
    "\n",
    "    def extract_feat(self, input_data):\n",
    "        \n",
    "        # put the model to GPU if it not there\n",
    "        if next(self.model.parameters()).device != input_data.device \\\n",
    "           or next(self.model.parameters()).dtype != input_data.dtype:\n",
    "            self.model.to(input_data.device, dtype=input_data.dtype)\n",
    "            self.model.train()\n",
    "\n",
    "        \n",
    "        if True:\n",
    "            # input should be in shape (batch, length)\n",
    "            if input_data.ndim == 3:\n",
    "                input_tmp = input_data[:, :, 0]\n",
    "            else:\n",
    "                input_tmp = input_data\n",
    "                \n",
    "            # [batch, length, dim]\n",
    "            emb = self.model(input_tmp, mask=False, features_only=True)['x']\n",
    "        return emb\n",
    "\n",
    "\n",
    "#---------AASIST back-end------------------------#\n",
    "''' Jee-weon Jung, Hee-Soo Heo, Hemlata Tak, Hye-jin Shim, Joon Son Chung, Bong-Jin Lee, Ha-Jin Yu and Nicholas Evans. \n",
    "    AASIST: Audio Anti-Spoofing Using Integrated Spectro-Temporal Graph Attention Networks. \n",
    "    In Proc. ICASSP 2022, pp: 6367--6371.'''\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_weight = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x   :(#bs, #node, #dim)\n",
    "        '''\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x)\n",
    "\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "        return x\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map(self, x):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "        att_map = torch.matmul(att_map, self.att_weight)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class HtrgGraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj_type1 = nn.Linear(in_dim, in_dim)\n",
    "        self.proj_type2 = nn.Linear(in_dim, in_dim)\n",
    "\n",
    "        # attention map\n",
    "        self.att_proj = nn.Linear(in_dim, out_dim)\n",
    "        self.att_projM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.att_weight11 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight22 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weight12 = self._init_new_params(out_dim, 1)\n",
    "        self.att_weightM = self._init_new_params(out_dim, 1)\n",
    "\n",
    "        # project\n",
    "        self.proj_with_att = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_att = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n",
    "        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # batch norm\n",
    "        self.bn = nn.BatchNorm1d(out_dim)\n",
    "\n",
    "        # dropout for inputs\n",
    "        self.input_drop = nn.Dropout(p=0.2)\n",
    "\n",
    "        # activate\n",
    "        self.act = nn.SELU(inplace=True)\n",
    "\n",
    "        # temperature\n",
    "        self.temp = 1.\n",
    "        if \"temperature\" in kwargs:\n",
    "            self.temp = kwargs[\"temperature\"]\n",
    "\n",
    "    def forward(self, x1, x2, master=None):\n",
    "        '''\n",
    "        x1  :(#bs, #node, #dim)\n",
    "        x2  :(#bs, #node, #dim)\n",
    "        '''\n",
    "        #print('x1',x1.shape)\n",
    "        #print('x2',x2.shape)\n",
    "        num_type1 = x1.size(1)\n",
    "        num_type2 = x2.size(1)\n",
    "        #print('num_type1',num_type1)\n",
    "        #print('num_type2',num_type2)\n",
    "        x1 = self.proj_type1(x1)\n",
    "        #print('proj_type1',x1.shape)\n",
    "        x2 = self.proj_type2(x2)\n",
    "        #print('proj_type2',x2.shape)\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        #print('Concat x1 and x2',x.shape)\n",
    "        \n",
    "        if master is None:\n",
    "            master = torch.mean(x, dim=1, keepdim=True)\n",
    "            #print('master',master.shape)\n",
    "        # apply input dropout\n",
    "        x = self.input_drop(x)\n",
    "\n",
    "        # derive attention map\n",
    "        att_map = self._derive_att_map(x, num_type1, num_type2)\n",
    "        #print('master',master.shape)\n",
    "        # directional edge for master node\n",
    "        master = self._update_master(x, master)\n",
    "        #print('master',master.shape)\n",
    "        # projection\n",
    "        x = self._project(x, att_map)\n",
    "        #print('proj x',x.shape)\n",
    "        # apply batch norm\n",
    "        x = self._apply_BN(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x1 = x.narrow(1, 0, num_type1)\n",
    "        #print('x1',x1.shape)\n",
    "        x2 = x.narrow(1, num_type1, num_type2)\n",
    "        #print('x2',x2.shape)\n",
    "        return x1, x2, master\n",
    "\n",
    "    def _update_master(self, x, master):\n",
    "\n",
    "        att_map = self._derive_att_map_master(x, master)\n",
    "        master = self._project_master(x, master, att_map)\n",
    "\n",
    "        return master\n",
    "\n",
    "    def _pairwise_mul_nodes(self, x):\n",
    "        '''\n",
    "        Calculates pairwise multiplication of nodes.\n",
    "        - for attention map\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, #dim)\n",
    "        '''\n",
    "\n",
    "        nb_nodes = x.size(1)\n",
    "        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n",
    "        x_mirror = x.transpose(1, 2)\n",
    "\n",
    "        return x * x_mirror\n",
    "\n",
    "    def _derive_att_map_master(self, x, master):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = x * master\n",
    "        att_map = torch.tanh(self.att_projM(att_map))\n",
    "\n",
    "        att_map = torch.matmul(att_map, self.att_weightM)\n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _derive_att_map(self, x, num_type1, num_type2):\n",
    "        '''\n",
    "        x           :(#bs, #node, #dim)\n",
    "        out_shape   :(#bs, #node, #node, 1)\n",
    "        '''\n",
    "        att_map = self._pairwise_mul_nodes(x)\n",
    "        # size: (#bs, #node, #node, #dim_out)\n",
    "        att_map = torch.tanh(self.att_proj(att_map))\n",
    "        # size: (#bs, #node, #node, 1)\n",
    "\n",
    "        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n",
    "\n",
    "        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n",
    "        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n",
    "        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n",
    "            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n",
    "        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n",
    "            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n",
    "\n",
    "        att_map = att_board\n",
    "\n",
    "        \n",
    "\n",
    "        # apply temperature\n",
    "        att_map = att_map / self.temp\n",
    "\n",
    "        att_map = F.softmax(att_map, dim=-2)\n",
    "\n",
    "        return att_map\n",
    "\n",
    "    def _project(self, x, att_map):\n",
    "        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n",
    "        x2 = self.proj_without_att(x)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _project_master(self, x, master, att_map):\n",
    "\n",
    "        x1 = self.proj_with_attM(torch.matmul(\n",
    "            att_map.squeeze(-1).unsqueeze(1), x))\n",
    "        x2 = self.proj_without_attM(master)\n",
    "\n",
    "        return x1 + x2\n",
    "\n",
    "    def _apply_BN(self, x):\n",
    "        org_size = x.size()\n",
    "        x = x.view(-1, org_size[-1])\n",
    "        x = self.bn(x)\n",
    "        x = x.view(org_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _init_new_params(self, *size):\n",
    "        out = nn.Parameter(torch.FloatTensor(*size))\n",
    "        nn.init.xavier_normal_(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n",
    "        super().__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n",
    "        self.in_dim = in_dim\n",
    "\n",
    "    def forward(self, h):\n",
    "        Z = self.drop(h)\n",
    "        weights = self.proj(Z)\n",
    "        scores = self.sigmoid(weights)\n",
    "        new_h = self.top_k_graph(scores, h, self.k)\n",
    "\n",
    "        return new_h\n",
    "\n",
    "    def top_k_graph(self, scores, h, k):\n",
    "        \"\"\"\n",
    "        args\n",
    "        =====\n",
    "        scores: attention-based weights (#bs, #node, 1)\n",
    "        h: graph data (#bs, #node, #dim)\n",
    "        k: ratio of remaining nodes, (float)\n",
    "        returns\n",
    "        =====\n",
    "        h: graph pool applied data (#bs, #node', #dim)\n",
    "        \"\"\"\n",
    "        _, n_nodes, n_feat = h.size()\n",
    "        n_nodes = max(int(n_nodes * k), 1)\n",
    "        _, idx = torch.topk(scores, n_nodes, dim=1)\n",
    "        idx = idx.expand(-1, -1, n_feat)\n",
    "\n",
    "        h = h * scores\n",
    "        h = torch.gather(h, 1, idx)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class Residual_block(nn.Module):\n",
    "    def __init__(self, nb_filts, first=False):\n",
    "        super().__init__()\n",
    "        self.first = first\n",
    "\n",
    "        if not self.first:\n",
    "            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n",
    "        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(1, 1),\n",
    "                               stride=1)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n",
    "        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n",
    "                               out_channels=nb_filts[1],\n",
    "                               kernel_size=(2, 3),\n",
    "                               padding=(0, 1),\n",
    "                               stride=1)\n",
    "\n",
    "        if nb_filts[0] != nb_filts[1]:\n",
    "            self.downsample = True\n",
    "            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n",
    "                                             out_channels=nb_filts[1],\n",
    "                                             padding=(0, 1),\n",
    "                                             kernel_size=(1, 3),\n",
    "                                             stride=1)\n",
    "\n",
    "        else:\n",
    "            self.downsample = False\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        if not self.first:\n",
    "            out = self.bn1(x)\n",
    "            out = self.selu(out)\n",
    "        else:\n",
    "            out = x\n",
    "\n",
    "        #print('out',out.shape)\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        #print('aft conv1 out',out.shape)\n",
    "        out = self.bn2(out)\n",
    "        out = self.selu(out)\n",
    "        # print('out',out.shape)\n",
    "        out = self.conv2(out)\n",
    "        #print('conv2 out',out.shape)\n",
    "        \n",
    "        if self.downsample:\n",
    "            identity = self.conv_downsample(identity)\n",
    "\n",
    "        out += identity\n",
    "        #out = self.mp(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # AASIST parameters\n",
    "        filts = [128, [1, 32], [32, 32], [32, 64], [64, 64]]\n",
    "        gat_dims = [64, 32]\n",
    "        pool_ratios = [0.5, 0.5, 0.5, 0.5]\n",
    "        temperatures =  [2.0, 2.0, 100.0, 100.0]\n",
    "\n",
    "        ####\n",
    "        # create network wav2vec 2.0\n",
    "        ####\n",
    "        self.ssl_model = SSLModel(self.device)\n",
    "        self.LL = nn.Linear(self.ssl_model.out_dim, 128)\n",
    "\n",
    "        self.first_bn = nn.BatchNorm2d(num_features=1)\n",
    "        self.first_bn1 = nn.BatchNorm2d(num_features=64)\n",
    "        self.drop = nn.Dropout(0.5, inplace=True)\n",
    "        self.drop_way = nn.Dropout(0.2, inplace=True)\n",
    "        self.selu = nn.SELU(inplace=True)\n",
    "\n",
    "        # RawNet2 encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[2])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[3])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])),\n",
    "            nn.Sequential(Residual_block(nb_filts=filts[4])))\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=(1,1)),\n",
    "            nn.SELU(inplace=True),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 64, kernel_size=(1,1)),   \n",
    "        )\n",
    "        # position encoding\n",
    "        self.pos_S = nn.Parameter(torch.randn(1, 42, filts[-1][-1]))\n",
    "        \n",
    "        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n",
    "        \n",
    "        # Graph module\n",
    "        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[0])\n",
    "        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n",
    "                                               gat_dims[0],\n",
    "                                               temperature=temperatures[1])\n",
    "        # HS-GAL layer \n",
    "        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n",
    "        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n",
    "            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n",
    "\n",
    "        # Graph pooling layers\n",
    "        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n",
    "        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n",
    "        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "\n",
    "        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n",
    "        \n",
    "        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #-------pre-trained Wav2vec model fine tunning ------------------------##\n",
    "        x_ssl_feat = self.ssl_model.extract_feat(x.squeeze(-1))\n",
    "        x = self.LL(x_ssl_feat) #(bs,frame_number,feat_out_dim)\n",
    "        \n",
    "        # post-processing on front-end features\n",
    "        x = x.transpose(1, 2)   #(bs,feat_out_dim,frame_number)\n",
    "        x = x.unsqueeze(dim=1) # add channel \n",
    "        x = F.max_pool2d(x, (3, 3))\n",
    "        x = self.first_bn(x)\n",
    "        x = self.selu(x)\n",
    "\n",
    "        # RawNet2-based encoder\n",
    "        x = self.encoder(x)\n",
    "        x = self.first_bn1(x)\n",
    "        x = self.selu(x)\n",
    "        \n",
    "        w = self.attention(x)\n",
    "        \n",
    "        #------------SA for spectral feature-------------#\n",
    "        w1 = F.softmax(w,dim=-1)\n",
    "        m = torch.sum(x * w1, dim=-1)\n",
    "        e_S = m.transpose(1, 2) + self.pos_S \n",
    "        \n",
    "        # graph module layer\n",
    "        gat_S = self.GAT_layer_S(e_S)\n",
    "        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n",
    "        \n",
    "        #------------SA for temporal feature-------------#\n",
    "        w2 = F.softmax(w,dim=-2)\n",
    "        m1 = torch.sum(x * w2, dim=-2)\n",
    "     \n",
    "        e_T = m1.transpose(1, 2)\n",
    "       \n",
    "        # graph module layer\n",
    "        gat_T = self.GAT_layer_T(e_T)\n",
    "        out_T = self.pool_T(gat_T)\n",
    "        \n",
    "        # learnable master node\n",
    "        master1 = self.master1.expand(x.size(0), -1, -1)\n",
    "        master2 = self.master2.expand(x.size(0), -1, -1)\n",
    "\n",
    "        # inference 1\n",
    "        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n",
    "            out_T, out_S, master=self.master1)\n",
    "\n",
    "        out_S1 = self.pool_hS1(out_S1)\n",
    "        out_T1 = self.pool_hT1(out_T1)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n",
    "            out_T1, out_S1, master=master1)\n",
    "        out_T1 = out_T1 + out_T_aug\n",
    "        out_S1 = out_S1 + out_S_aug\n",
    "        master1 = master1 + master_aug\n",
    "\n",
    "        # inference 2\n",
    "        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n",
    "            out_T, out_S, master=self.master2)\n",
    "        out_S2 = self.pool_hS2(out_S2)\n",
    "        out_T2 = self.pool_hT2(out_T2)\n",
    "\n",
    "        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n",
    "            out_T2, out_S2, master=master2)\n",
    "        out_T2 = out_T2 + out_T_aug\n",
    "        out_S2 = out_S2 + out_S_aug\n",
    "        master2 = master2 + master_aug\n",
    "\n",
    "        out_T1 = self.drop_way(out_T1)\n",
    "        out_T2 = self.drop_way(out_T2)\n",
    "        out_S1 = self.drop_way(out_S1)\n",
    "        out_S2 = self.drop_way(out_S2)\n",
    "        master1 = self.drop_way(master1)\n",
    "        master2 = self.drop_way(master2)\n",
    "\n",
    "        out_T = torch.max(out_T1, out_T2)\n",
    "        out_S = torch.max(out_S1, out_S2)\n",
    "        master = torch.max(master1, master2)\n",
    "\n",
    "        # Readout operation\n",
    "        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n",
    "        T_avg = torch.mean(out_T, dim=1)\n",
    "\n",
    "        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n",
    "        S_avg = torch.mean(out_S, dim=1)\n",
    "        \n",
    "        last_hidden = torch.cat(\n",
    "            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n",
    "        \n",
    "        last_hidden = self.drop(last_hidden)\n",
    "        output = self.out_layer(last_hidden)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "def get_model(path_pth, device):\n",
    "    model = Model(device)\n",
    "    model.load_state_dict(torch.load(path_pth,map_location=device))\n",
    "\n",
    "    nb_params = sum([param.view(-1).size()[0] for param in model.parameters()])\n",
    "    model = model.to(device)\n",
    "    print('nb_params:',nb_params)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressbar(it, prefix=\"\", size=60, out=sys.stdout):  # Python3.6+\n",
    "    count = len(it)\n",
    "    start = time.time()\n",
    "\n",
    "    def show(j):\n",
    "        x = int(size * j / count)\n",
    "        remaining = ((time.time() - start) / j) * (count - j)\n",
    "        passing = time.time() - start\n",
    "        mins_pas, sec_pass = divmod(passing, 60)\n",
    "        time_pas = f\"{int(mins_pas):02}:{sec_pass:05.2f}\"\n",
    "\n",
    "        mins, sec = divmod(remaining, 60)\n",
    "        time_str = f\"{int(mins):02}:{sec:05.2f}\"\n",
    "\n",
    "        print(f\"{prefix}[{u'█' * x}{('.' * (size - x))}] {j}/{count} time {time_pas} / {time_str}\", end='\\r', file=out,\n",
    "              flush=True)\n",
    "\n",
    "    for i, item in enumerate(it):\n",
    "        yield item\n",
    "        show(i + 1)\n",
    "    print(\"\\n\", flush=True, file=out)\n",
    "\n",
    "\n",
    "class ChanelWiseStats(nn.Module):\n",
    "    \"\"\"\n",
    "    The class that computes mean and standart deviation\n",
    "    in input data acrocc channels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ChanelWiseStats, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.data.shape[0], x.data.shape[1],\n",
    "                   x.data.shape[2] * x.data.shape[3])\n",
    "\n",
    "        mean = torch.mean(x, 2)\n",
    "        std = torch.std(x, 2)\n",
    "\n",
    "        return torch.stack((mean, std), dim=1)\n",
    "\n",
    "\n",
    "class View(nn.Module):\n",
    "    \"\"\"\n",
    "    Auxiliary class\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input.view(self.shape)\n",
    "\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "def pad_random(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "\n",
    "    if x_len > max_len:\n",
    "        stt = np.random.randint(x_len - max_len)\n",
    "        return x[stt:stt + max_len]\n",
    "\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, num_repeats)[:max_len]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def pad(x, max_len=64600):\n",
    "    x_len = x.shape[0]\n",
    "    if x_len >= max_len:\n",
    "        return x[:max_len]\n",
    "    # need to pad\n",
    "    num_repeats = int(max_len / x_len) + 1\n",
    "    padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n",
    "    return padded_x\n",
    "\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    if config[\"opt\"] == 'Adam':\n",
    "        optimizer = Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        )\n",
    "    elif config[\"opt\"] == 'AdaBound':\n",
    "        optimizer = AdaBound(\n",
    "            model.parameters(),\n",
    "            lr=config['lr'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Optimizer {config['optimizer']} not supported\")\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    # with open(path, \"r\") as f:\n",
    "    #     return json.load(f)\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVspoof2019(Dataset):\n",
    "    def __init__(self, ids, dir_path, labels, pad_fn=pad_random, is_train=True):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.dir_path = dir_path\n",
    "        self.cut = 64600\n",
    "        self.is_train = is_train\n",
    "        self.pad_fn = pad_fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_to_flac = f\"{self.dir_path}/flac/{self.ids[index]}.flac\"\n",
    "        audio, rate = sf.read(path_to_flac)\n",
    "        x_pad = self.pad_fn(audio, self.cut)\n",
    "        x_inp = Tensor(x_pad)\n",
    "        if not self.is_train:\n",
    "            return x_inp, self.ids[index], torch.tensor(self.labels[index])\n",
    "        return x_inp, torch.tensor(self.labels[index]), rate\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, ids, dir_path, pad_fn=pad_random, cut=64600):\n",
    "        self.ids = ids\n",
    "        self.dir_path = dir_path\n",
    "        self.cut = cut\n",
    "        self.pad_fn = pad_fn\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path_to_wav = f\"{self.dir_path}/{self.ids[index]}\"\n",
    "        audio, rate = sf.read(path_to_wav)\n",
    "        x_pad = self.pad_fn(audio, self.cut)\n",
    "        x_inp = Tensor(x_pad)\n",
    "        return x_inp, self.ids[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def get_data_for_evaldataset(path):\n",
    "    ids_list = os.listdir(path)\n",
    "    return ids_list\n",
    "\n",
    "\n",
    "def get_data_for_dataset(path):\n",
    "    ids_list = []\n",
    "    label_list = []\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.split()\n",
    "            id, label = line[1], line[-1]\n",
    "            ids_list.append(id)\n",
    "            label = 1 if label == \"bonafide\" else 0\n",
    "            label_list.append(label)\n",
    "    return ids_list, label_list\n",
    "\n",
    "\n",
    "def get_datasets(config):\n",
    "    if config[\"model\"] == \"Res2TCNGuard\":\n",
    "        val_pad_fn = pad\n",
    "    else:\n",
    "        val_pad_fn = pad_random\n",
    "\n",
    "    train_ids, train_labels = get_data_for_dataset(config[\"train_label_path\"])\n",
    "    train_dataset = ASVspoof2019(\n",
    "        train_ids,\n",
    "        config[\"train_path_flac\"],\n",
    "        train_labels\n",
    "    )\n",
    "\n",
    "    dev_ids, dev_labels = get_data_for_dataset(config[\"dev_label_path\"])\n",
    "    dev_dataset = ASVspoof2019(\n",
    "        dev_ids,\n",
    "        config[\"dev_path_flac\"],\n",
    "        dev_labels,\n",
    "        val_pad_fn,\n",
    "        False\n",
    "    )\n",
    "\n",
    "    eval_ids, eval_labels = get_data_for_dataset(config[\"eval_label_path\"])\n",
    "\n",
    "    eval_dataset = ASVspoof2019(eval_ids, config[\"eval_path_flac\"], eval_labels, val_pad_fn, False)\n",
    "\n",
    "    return {\n",
    "        \"train\": train_dataset,\n",
    "        \"dev\": dev_dataset,\n",
    "        \"eval\": eval_dataset\n",
    "    }\n",
    "\n",
    "\n",
    "def get_dataloaders(datasets, config):\n",
    "    dataloaders = {}\n",
    "\n",
    "    if datasets.get(\"train\"):\n",
    "        train_loader = DataLoader(\n",
    "            datasets[\"train\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=True,\n",
    "            num_workers=config[\"num_workers\"]\n",
    "        )\n",
    "        dataloaders[\"train\"] = train_loader\n",
    "    if datasets.get(\"dev\"):\n",
    "        dev_loader = DataLoader(\n",
    "            datasets[\"dev\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=config[\"num_workers\"]\n",
    "        )\n",
    "        dataloaders[\"dev\"] = dev_loader\n",
    "\n",
    "    if datasets.get(\"eval\"):\n",
    "        eval_loader = DataLoader(\n",
    "            datasets[\"eval\"],\n",
    "            batch_size=config[\"batch_size\"],\n",
    "            shuffle=False,\n",
    "            num_workers=config[\"num_workers\"]\n",
    "        )\n",
    "        dataloaders[\"eval\"] = eval_loader\n",
    "\n",
    "    return dataloaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_det_curve(bonafide_scores, spoof_scores):\n",
    "    \"\"\"\n",
    "    function, that comuputes FRR and FAR with their thresholds\n",
    "\n",
    "    args:\n",
    "        bonafide_scores: score for bonafide speech\n",
    "        spoof_scores: score for spoofed speech\n",
    "    output:\n",
    "        frr: false rejection rate\n",
    "        far: false acceptance rate\n",
    "        threshlods: thresholds for frr and far\n",
    "    todo:\n",
    "        rewrite to torch\n",
    "        create tests\n",
    "    \"\"\"\n",
    "    # number of scores\n",
    "    n_scores = bonafide_scores.size + spoof_scores.size\n",
    "\n",
    "    # bona fide scores and spoof scores\n",
    "    all_scores = np.concatenate((bonafide_scores, spoof_scores))\n",
    "\n",
    "    # label of bona fide score is 1\n",
    "    # label of spoof score is 0\n",
    "    labels = np.concatenate((np.ones(bonafide_scores.size), np.zeros(spoof_scores.size)))\n",
    "\n",
    "    # indexes of sorted scores in all scores\n",
    "    indices = np.argsort(all_scores, kind='mergesort')\n",
    "    # sort labels based on scores\n",
    "    labels = labels[indices]\n",
    "\n",
    "    # Compute false rejection and false acceptance rates\n",
    "\n",
    "    # tar cumulative value\n",
    "    tar_trial_sums = np.cumsum(labels)\n",
    "    nontarget_trial_sums = spoof_scores.size - (np.arange(1, n_scores + 1) - tar_trial_sums)\n",
    "\n",
    "    # false rejection rates\n",
    "    frr = np.concatenate((np.atleast_1d(0), tar_trial_sums / bonafide_scores.size))\n",
    "\n",
    "    # false acceptance rates\n",
    "    far = np.concatenate((np.atleast_1d(1), nontarget_trial_sums / spoof_scores.size))\n",
    "\n",
    "    # Thresholds are the sorted scores\n",
    "    thresholds = np.concatenate((np.atleast_1d(all_scores[indices[0]] - 0.001), all_scores[indices]))\n",
    "\n",
    "    return frr, far, thresholds\n",
    "\n",
    "\n",
    "def compute_eer(bonafide_scores, spoof_scores):\n",
    "    \"\"\"\n",
    "    Returns equal error rate (EER) and the corresponding threshold.\n",
    "    args:\n",
    "        bonafide_scores: score for bonafide speech\n",
    "        spoof_scores: score for spoofed speech\n",
    "    output:\n",
    "        eer: equal error rate\n",
    "        threshold: index, where frr=far\n",
    "    todo:\n",
    "        rewrite to torch\n",
    "        create tests\n",
    "    \"\"\"\n",
    "    frr, far, thresholds = compute_det_curve(bonafide_scores, spoof_scores)\n",
    "\n",
    "    # absolute differense between frr and far\n",
    "    abs_diffs = np.abs(frr - far)\n",
    "\n",
    "    # index of minimal absolute difference\n",
    "    min_index = np.argmin(abs_diffs)\n",
    "\n",
    "    # equal error rate\n",
    "    eer = np.mean((frr[min_index], far[min_index]))\n",
    "    return eer, thresholds[min_index]\n",
    "\n",
    "\n",
    "# @torch.inference_mode\n",
    "def produce_evaluation_file(data_loader,\n",
    "                            model,\n",
    "                            device,\n",
    "                            loss_fn,\n",
    "                            save_path,\n",
    "                            trial_path,\n",
    "                            random=False,\n",
    "                            dropout=0):\n",
    "    \"\"\"\n",
    "    Create file, that need to give in function calculcate_t-DCF_EER\n",
    "    args:\n",
    "        data_loader: loader, that gives batch to model\n",
    "        model: model, that calculate what we need\n",
    "        device: device for data, model\n",
    "        save_path: path where file shoud be saved\n",
    "        trial_path: path from LA CM protocols\n",
    "    todo:\n",
    "        this function must return result: tensor of uid, src, key, score\n",
    "    \"\"\"\n",
    "\n",
    "    # turning model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # read file ASVspoof2019.LA.cm.<dev/train/eval>.trl.txt\n",
    "    with open(trial_path, \"r\") as file_trial:\n",
    "        trial_lines = file_trial.readlines()\n",
    "\n",
    "    # list of utterance id and list of score for appropiate uid\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    current_loss = 0\n",
    "    # inference\n",
    "    for batch_x, utt_id, batch_y in progressbar(data_loader, prefix='computing cm score'):\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        with torch.no_grad():\n",
    "            # first is hidden layer, second is result\n",
    "            batch_out = model(batch_x)\n",
    "            # 1 - for bonafide speech class\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "            loss = loss_fn(batch_out, batch_y)\n",
    "            current_loss += loss.item() / len(data_loader)\n",
    "\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "    # assert len(trial_lines) == len(fname_list) == len(score_list)\n",
    "\n",
    "    # saving results\n",
    "    with open(save_path, \"w\") as fh:\n",
    "\n",
    "        # fn - uid, sco - score, trl - trial_lines\n",
    "        for fn, sco, trl in zip(fname_list, score_list, trial_lines):\n",
    "            _, utt_id, _, src, key = trl.strip().split(' ')\n",
    "            assert fn == utt_id\n",
    "            # format: utterance id - type of spoof attack - key - score\n",
    "            fh.write(\"{} {} {} {}\\n\".format(utt_id, src, key, sco))\n",
    "    print(\"Scores saved to {}\".format(save_path))\n",
    "\n",
    "    return current_loss\n",
    "\n",
    "# @torch.inference_mode\n",
    "def produce_submit_file(data_loader,\n",
    "                            model,\n",
    "                            device,\n",
    "                            save_path,\n",
    "                            random=False,\n",
    "                            dropout=0):\n",
    "    \"\"\"\n",
    "    Create file, that need to give in function calculcate_t-DCF_EER\n",
    "    args:\n",
    "        data_loader: loader, that gives batch to model\n",
    "        model: model, that calculate what we need\n",
    "        device: device for data, model\n",
    "        save_path: path where file shoud be saved\n",
    "    \"\"\"\n",
    "\n",
    "    # turning model into evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # list of utterance id and list of score for appropiate uid\n",
    "    fname_list = []\n",
    "    score_list = []\n",
    "    # inference\n",
    "    for batch_x, utt_id in progressbar(data_loader, prefix='computing cm score'):\n",
    "        batch_x = batch_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            # first is hidden layer, second is result\n",
    "            batch_out = model(batch_x)\n",
    "            # 1 - for bonafide speech class\n",
    "            batch_score = (batch_out[:, 1]).data.cpu().numpy().ravel()\n",
    "\n",
    "        # add outputs\n",
    "        fname_list.extend(utt_id)\n",
    "        score_list.extend(batch_score.tolist())\n",
    "    assert len(fname_list) == len(score_list)\n",
    "\n",
    "    # saving results\n",
    "    with open(save_path, \"w\") as fh:\n",
    "        for fn, sco in zip(fname_list, score_list):\n",
    "            if \".wav\" in fn:\n",
    "                fn = fn.replace(\".wav\", \"\")\n",
    "            fh.write(\"{} {}\\n\".format(fn, sco))\n",
    "    df = pd.read_csv(save_path, sep=\" \", names=[\"ID\", \"score\"])\n",
    "    df.to_csv(save_path, index=False)\n",
    "    print(\"Scores saved to {}\".format(save_path))\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def obtain_asv_error_rates(tar_asv, non_asv, spoof_asv, asv_thresholds):\n",
    "    \"\"\"\n",
    "    Calculate false alarm rate and miss rate for asv scores\n",
    "\n",
    "    args:\n",
    "        tar_asv: scores for asv targets\n",
    "        non_asv: scores for asv nontargets\n",
    "        spoof_asv: scores for asv spoofed\n",
    "        asv_threshold: threshold for asv EER between targets and non_targets\n",
    "    returns:\n",
    "        Pfa_asv: false alarm rate for asv\n",
    "        Pmiss_asv: false miss rate for asv\n",
    "        Pmiss_spoof_asv: rate of rejection spoofs in asv\n",
    "    todo:\n",
    "        rewrite to torch\n",
    "    \"\"\"\n",
    "    Pfa_asv = sum(non_asv >= asv_thresholds) / non_asv.size\n",
    "    Pmiss_asv = sum(tar_asv < asv_thresholds) / tar_asv.size\n",
    "\n",
    "    if spoof_asv.size == 0:\n",
    "        Pmiss_spoof_asv = None\n",
    "    else:\n",
    "        Pmiss_spoof_asv = np.sum(spoof_asv < asv_thresholds) / spoof_asv.size\n",
    "\n",
    "    return Pfa_asv, Pmiss_asv, Pmiss_spoof_asv\n",
    "\n",
    "\n",
    "def compute_tDCF(bonafide_score_cm, spoof_score_cm, Pfa_asv,\n",
    "                 Pmiss_asv, Pmiss_spoof_asv, cost_model):\n",
    "    \"\"\"\n",
    "    This function computes min t-DCF value\n",
    "\n",
    "    args:\n",
    "        bonafide_score_cm: score for bonafide speech from CM system\n",
    "        spoof_score_cm: score for spoofed speech from CM systn\n",
    "        Pfa_asv: false alarm rate from asv system\n",
    "        Pmiss_asv: miss rate from asv sustem\n",
    "        Pmiss_spoof_asv: miss rate for spoof utterance from asv system\n",
    "        cost_model: dict of parameters for t-DCF\n",
    "    output:\n",
    "        t-DCF: computed value\n",
    "        CM_threshold: threshold for EER between Pmiss_cm and Pfa_cm\n",
    "    todo:\n",
    "        rewrite to torch\n",
    "    \"\"\"\n",
    "\n",
    "    # obtain miss and false alarm rate of cm\n",
    "    Pmiss_cm, Pfa_cm, CM_thresholds = compute_det_curve(\n",
    "        bonafide_score_cm, spoof_score_cm\n",
    "    )\n",
    "\n",
    "    # Constants\n",
    "    C1 = cost_model['Ptar'] * (cost_model['Cmiss_cm'] - cost_model['Cmiss_asv'] * Pmiss_asv) - \\\n",
    "         cost_model['Pnon'] * cost_model['Cfa_asv'] * Pfa_asv\n",
    "\n",
    "    C2 = cost_model['Cfa_cm'] * cost_model['Pspoof'] * (1 - Pmiss_spoof_asv)\n",
    "\n",
    "    # obtain t-DCF curve for all thresholds\n",
    "    tDCF = C1 * Pmiss_cm + C2 * Pfa_cm\n",
    "\n",
    "    # normalized t-DCF\n",
    "    tDCFnorm = tDCF / np.minimum(C1, C2)\n",
    "\n",
    "    return tDCFnorm, CM_thresholds\n",
    "\n",
    "\n",
    "def calculate_eer_tdcf(cm_scores_file, asv_score_file, output_file, printout=True):\n",
    "    \"\"\"\n",
    "    Function cimputes tdcf, eer for CM sustem, and also compute\n",
    "    EER of each type of attack and write them into file\n",
    "    args:\n",
    "        cm_scores_file: file from produce_evaluation file\n",
    "        asv_score_file: file from organizers\n",
    "        ouput_file: file where information of each type of attack for eval dataset will be\n",
    "        printout: print this file or not\n",
    "    output:\n",
    "        EER * 100: percentage of equal error rate for CM system\n",
    "        min_tDCF: value of t-DCF for CM system\n",
    "    todo:\n",
    "        rewrite into torch\n",
    "        return array instead of create file\n",
    "    \"\"\"\n",
    "    # cm data from file\n",
    "    cm_data = np.genfromtxt(cm_scores_file, dtype=str)\n",
    "\n",
    "    # type of spoof attack\n",
    "    cm_sources = cm_data[:, 1]\n",
    "\n",
    "    # spoof or bonafide speech\n",
    "    cm_keys = cm_data[:, 2]\n",
    "\n",
    "    # score for utterance\n",
    "    cm_scores = cm_data[:, 3].astype(np.float64)\n",
    "\n",
    "    # score for bonafide speech\n",
    "    bona_cm = cm_scores[cm_keys == 'bonafide']\n",
    "\n",
    "    # score for spoofed utterance\n",
    "    spoof_cm = cm_scores[cm_keys == 'spoof']\n",
    "\n",
    "    # equal error rate\n",
    "    EER, _ = compute_eer(bona_cm, spoof_cm)\n",
    "\n",
    "    # fix parameters for t-DCF\n",
    "    cost_model = {\n",
    "        'Pspoof': 0.05,\n",
    "        'Ptar': 0.9405,\n",
    "        'Pnon': 0.0095,\n",
    "        'Cmiss': 1,\n",
    "        'Cfa': 10,  ###########\n",
    "        'Cmiss_asv': 1,\n",
    "        'Cfa_asv': 10,\n",
    "        'Cmiss_cm': 1,\n",
    "        'Cfa_cm': 10,\n",
    "    }\n",
    "\n",
    "    # load organizers' ASV scores\n",
    "    asv_data = np.genfromtxt(asv_score_file, dtype=str)\n",
    "\n",
    "    # keys: target, non-target, spoof\n",
    "    asv_keys = asv_data[:, 1]\n",
    "\n",
    "    # score for each utterance\n",
    "    asv_scores = asv_data[:, 2].astype(np.float64)\n",
    "\n",
    "    # target, non-target and spoof scores from the ASV scores\n",
    "    tar_asv = asv_scores[asv_keys == 'target']\n",
    "    non_asv = asv_scores[asv_keys == 'nontarget']\n",
    "    spoof_asv = asv_scores[asv_keys == 'spoof']\n",
    "\n",
    "    # EER of the standalone systems and fix ASV operation point to\n",
    "    eer_asv, asv_threshold = compute_eer(tar_asv, non_asv)\n",
    "\n",
    "    # generate attack types from A07 to A19\n",
    "    attack_types = [f'A{_id:02d}' for _id in range(7, 20)]\n",
    "\n",
    "    # compute eer for each type of attack\n",
    "    if printout:\n",
    "        spoof_cm_breakdown = {\n",
    "            attack_type: cm_scores[cm_sources == attack_type]\n",
    "            for attack_type in attack_types\n",
    "        }\n",
    "\n",
    "        eer_cm_breakdown = {\n",
    "            attack_type: compute_eer(bona_cm, spoof_cm_breakdown[attack_type])[0]\n",
    "            for attack_type in attack_types\n",
    "        }\n",
    "    [Pfa_asv, Pmiss_asv, Pmiss_spoof_asv] = obtain_asv_error_rates(\n",
    "        tar_asv,\n",
    "        non_asv,\n",
    "        spoof_asv,\n",
    "        asv_threshold\n",
    "    )\n",
    "\n",
    "    # Compute t-DCF\n",
    "    tDCF_curve, CM_thresholds = compute_tDCF(\n",
    "        bona_cm,\n",
    "        spoof_cm,\n",
    "        Pfa_asv,\n",
    "        Pmiss_asv,\n",
    "        Pmiss_spoof_asv,\n",
    "        cost_model\n",
    "    )\n",
    "\n",
    "    # Minimum t-DCF\n",
    "    min_tDCF_index = np.argmin(tDCF_curve)\n",
    "    min_tDCF = tDCF_curve[min_tDCF_index]\n",
    "    # write results into file\n",
    "    if printout:\n",
    "        with open(output_file, 'w') as f_res:\n",
    "            f_res.write('\\nCM SYSTEM\\n')\n",
    "            f_res.write(\"\"\"\\tEER\\t\\t= {:8.9f} % \n",
    "            (Equal error rate for countermeasure)\\n\"\"\".format(EER * 100)\n",
    "                        )\n",
    "            f_res.write('\\nTANDEM\\n')\n",
    "            f_res.write('\\tmin-tDCF\\t\\t= {:8.9f}\\n'.format(min_tDCF))\n",
    "            f_res.write('\\nBREAKDOWN CM SYSTEM\\n')\n",
    "            for attack_type in attack_types:\n",
    "                _eer = eer_cm_breakdown[attack_type] * 100\n",
    "                f_res.write(\n",
    "                    f'\\tEER {attack_type}\\t\\t= {_eer:8.9f} % (Equal error rate for {attack_type})\\n'\n",
    "                )\n",
    "        os.system(f\"cat {output_file}\")\n",
    "    return EER * 100, min_tDCF\n",
    "\n",
    "\n",
    "def evaluate_EER_file(ref_df, pred_df, output_file):\n",
    "    \"\"\"\n",
    "\n",
    "        :param ref_df: csv file with columns: uttid, label\n",
    "        :param pred_df: csv file with columns: uttid, score\n",
    "        :return: err\n",
    "        \"\"\"\n",
    "\n",
    "    ref_df = pd.read_csv(ref_df, header=None, names=[\"_\", \"uttid\", \"___\", \"__\", \"label\"], sep=\" \")\n",
    "    ref_df = ref_df.sort_values(\"uttid\")\n",
    "\n",
    "    pred_df = pd.read_csv(pred_df, header=None, names=[\"uttid\", \"_\", \"__\", \"scores\"], sep=\" \")\n",
    "    pred_df = pred_df.sort_values(\"uttid\")\n",
    "    if not ref_df[\"uttid\"].equals(pred_df[\"uttid\"]):\n",
    "        raise ValueError(\"The 'uttid' columns in the reference and prediction files do not match.\")\n",
    "\n",
    "    pos_scores = pred_df[\"scores\"][ref_df[\"label\"] == \"bonafide\"]\n",
    "    neg_scores = pred_df[\"scores\"][ref_df[\"label\"] == \"spoof\"]\n",
    "\n",
    "    eer, _ = compute_eer(pos_scores, neg_scores)\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(f\"EER: {eer}\")\n",
    "    return eer * 100\n",
    "\n",
    "\n",
    "def evaluate_EER(ref_df, pred_df):\n",
    "    \"\"\"\n",
    "\n",
    "    :param ref_df: csv file with columns: uttid, label\n",
    "    :param pred_df: csv file with columns: uttid, score\n",
    "    :return: err\n",
    "    \"\"\"\n",
    "\n",
    "    ref_df = pd.read_csv(ref_df, header=None, names=[\"speaker\", \"uttid\", \"-\", \"algo\", \"label\"], sep=\" \")\n",
    "    ref_df = ref_df.sort_values(\"uttid\")\n",
    "\n",
    "    pred_df = pd.read_csv(pred_df, header=None, names=[\"uttid\", \"algo\", \"label\", \"scores\"], sep=\" \")\n",
    "    pred_df = pred_df.sort_values(\"uttid\")\n",
    "\n",
    "    if not ref_df[\"uttid\"].equals(pred_df[\"uttid\"]):\n",
    "        raise ValueError(\"The 'uttid' columns in the reference and prediction files do not match.\")\n",
    "\n",
    "    pos_scores = pred_df[\"scores\"][ref_df[\"label\"] == 'bonafide']\n",
    "    neg_scores = pred_df[\"scores\"][ref_df[\"label\"] == 'spoof']\n",
    "\n",
    "    eer, _ = compute_eer(pos_scores, neg_scores)\n",
    "    return eer * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_params: 317837834\n",
      "computing cm score[............................................................] 286/18087 time 00:23.55 / 24:25.86\r"
     ]
    }
   ],
   "source": [
    "def submit(cfg=CONFIG, eval_path_wav=\"/asvspoof/wavs/\", output_file=\"submit.csv\"):\n",
    "    eval_ids = get_data_for_evaldataset(eval_path_wav)\n",
    "\n",
    "    eval_dataset = EvalDataset(eval_ids, eval_path_wav, pad)\n",
    "    eval_dataset = {\n",
    "        \"eval\": eval_dataset\n",
    "    }\n",
    "\n",
    "    dataloader = get_dataloaders(eval_dataset, cfg)\n",
    "\n",
    "    model = get_model(cfg[\"checkpoint\"], cfg[\"device\"])\n",
    "\n",
    "    produce_submit_file(\n",
    "        dataloader[\"eval\"],\n",
    "        model,\n",
    "        cfg[\"device\"],\n",
    "        output_file\n",
    "    )\n",
    "\n",
    "submit(cfg=CONFIG, eval_path_wav=\"/asvspoof/wavs/\", output_file=\"submit_1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
